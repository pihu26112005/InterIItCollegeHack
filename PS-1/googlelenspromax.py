# -*- coding: utf-8 -*-
"""Copy of GoogleLensProMax.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RXPwUzog0CuLcFMFfCwt8XSQFg-VsGMq
"""

from huggingface_hub import notebook_login

notebook_login()

from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import requests
from google.colab import files

# Load the BLIP model and processor
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model2 = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

# Upload an image
uploaded = files.upload()

# Load the image
image_path = next(iter(uploaded))
image = Image.open(image_path)

# Preprocess the image
inputs = processor(images=image, return_tensors="pt")

# Generate the caption
out = model2.generate(**inputs)

# Decode the generated caption
caption = processor.decode(out[0], skip_special_tokens=True)
caption

user_input = input("Enter additional text to describe your intent: ")

!pip install -U bitsandbytes

!pip install -q -U transformers accelerate

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from accelerate import init_empty_weights, load_checkpoint_and_dispatch

# Enable GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load the tokenizer
model_id = "meta-llama/Meta-Llama-3.1-8B-Instruct"  # Replace with the correct model
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Quantization config
bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True)

# Initialize model in 4-bit precision
with init_empty_weights():
    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map="auto", torch_dtype=torch.float16 )

# Refine the query based on the generated caption and intent
final_query = f"Context: {caption}. User Query: {user_input}. Instruction: Generate a search query based mainly on context and type of website to search."
# final_query = [{"context" : caption, "query" : user_input, "message": "Generate a search query based on users intent. You need to output only the final query and nothing else. Also try to add a snipped at the end of your answer showing what kind of websites my search engine needs to search. Add this snippet to the end of the search sentence you formed", }]
input_ids = tokenizer(final_query, return_tensors="pt").input_ids.to(device)

# Generate text
# outputs = model.generate(input_ids, max_new_tokens=256)
outputs = model.generate(
    input_ids,
    max_new_tokens=150,  # Limit the number of tokens
    temperature=0.4,    # Lower temperature for more deterministic, concise output
    do_sample=True,    # Use greedy decoding for more focused generation
)
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(generated_text)

import re
matches = re.findall(r'"(.*?)"', generated_text)

# Print only the first match or a default message if no matches found
final_output = matches[0] if matches else "No valid query found."
print(final_output)

def perform_search(query, serpapi_key):
    params = {
        "engine": "duckduckgo",
        "q": f'{query}',
        "api_key": serpapi_key,
        "num": 10,
        "hl": "en",
        "gl": "us"
    }
    search_url = "https://serpapi.com/search"
    response = requests.get(search_url, params=params)
    if response.status_code == 200:
        results = response.json()
        return results.get('organic_results', [])
    else:
        print("Error in API request:", response.status_code, response.text)
        return []
SERPAPI_KEY = "31dcf489be94603d1b32ab99f4c19e92f6333fb6e8c53ddf170d9b08548514ba"  # Replace with your actual SerpAPI key
search_results = perform_search(final_output, SERPAPI_KEY)
search_results

"""so basically click on link under every tittle to see the the results of the search query

"""